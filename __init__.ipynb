{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import io\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    print('Completed resource downloads.')\n",
    "\n",
    "def filter_for_tags(tagged,tags=['NN','JJ','NNP']):                         #NN for noun,NNP for proper noun and JJ for adjective\n",
    "    return [item for item in tagged if item[1] in tags]\n",
    "\n",
    "def normalize(tagged):\n",
    "    return [(item[0].replace('.',''),item[1]) for item in tagged]\n",
    "\n",
    "def unique_everseen(iterable, key=None):\n",
    "    \"\"\"List unique elements in order of appearance.\n",
    "       Examples: unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
    "                 unique_everseen('ABBCcAD', str.lower) --> A B C D  \"\"\"\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in [x for x in iterable if x not in seen]:\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    "\n",
    "def build_graph(nodes):\n",
    "    gr = nx.Graph()\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "    \n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        levDistance = editdistance.eval(firstString ,secondString)\n",
    "        gr.add_edge(firstString ,secondString ,weight=levDistance)\n",
    "        \n",
    "    return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_phrases(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(word_tokens)\n",
    "    textlist = [x[0] for x in tagged]\n",
    "    tagged = filter_for_tags(tagged)\n",
    "    tagged = normalize(tagged)\n",
    "    unique_word_set = unique_everseen([x[0] for x in tagged])\n",
    "    word_set_list = list(unique_word_set)\n",
    "    graph = build_graph(word_set_list)\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get,reverse=True)\n",
    "    one_third = len(word_set_list) // 3\n",
    "    keyphrases = keyphrases[0:one_third + 1]\n",
    "    modified_key_phrases = set([])\n",
    "    dealt_with = set([])\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(textlist):\n",
    "        first = textlist[i]\n",
    "        second = textlist[j]\n",
    "        if first in keyphrases and second in keyphrases:\n",
    "            keyphrase = first + ' ' + second\n",
    "            modified_key_phrases.add(keyphrase)\n",
    "            dealt_with.add(first)\n",
    "            dealt_with.add(second)\n",
    "        else:\n",
    "            if first in keyphrases and first not in dealt_with:\n",
    "                modified_key_phrases.add(first)\n",
    "            if j == len(textlist) - 1 and second in keyphrases and second not in dealt_with:\n",
    "                modified_key_phrases.add(second)\n",
    "\n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "\n",
    "    return modified_key_phrases\n",
    "\n",
    "def extract_sentences(text, summary_length=100, clean_sentences=False, language='english'):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/'+language+'.pickle')\n",
    "    sentence_tokens = sent_detector.tokenize(text.strip())\n",
    "    graph = build_graph(sentence_tokens)\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get,reverse=True)\n",
    "    summary = ' '.join(sentences)\n",
    "    summary_words = summary.split()\n",
    "    summary_words = summary_words[0:summary_length]\n",
    "    dot_indices = [idx for idx, word in enumerate(summary_words) if word.find('.') != -1]\n",
    "    if clean_sentences and dot_indices:\n",
    "        last_dot = max(dot_indices) + 1\n",
    "        summary = ' '.join(summary_words[0:last_dot])\n",
    "    else:\n",
    "        summary = ' '.join(summary_words)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def write_files(summary, key_phrases, filename):\n",
    "    print(\"Generating output to \" + 'keywords/' + filename)\n",
    "    key_phrase_file = io.open('keywords/' + filename, 'w',encoding=\"utf8\")\n",
    "    for key_phrase in key_phrases:\n",
    "        key_phrase_file.write(key_phrase + '\\n')\n",
    "    key_phrase_file.close()\n",
    "\n",
    "    print(\"Generating output to \" + 'summaries/' + filename)\n",
    "    summary_file = io.open('summaries/' + filename, 'w',encoding=\"utf8\")\n",
    "    summary_file.write(summary)\n",
    "    summary_file.close()\n",
    "\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating output to keywords/1.txt\n",
      "Generating output to summaries/1.txt\n",
      "-\n",
      "Generating output to keywords/10.txt\n",
      "Generating output to summaries/10.txt\n",
      "-\n",
      "Generating output to keywords/2.txt\n",
      "Generating output to summaries/2.txt\n",
      "-\n",
      "Generating output to keywords/3.txt\n",
      "Generating output to summaries/3.txt\n",
      "-\n",
      "Generating output to keywords/4.txt\n",
      "Generating output to summaries/4.txt\n",
      "-\n",
      "Generating output to keywords/5.txt\n",
      "Generating output to summaries/5.txt\n",
      "-\n",
      "Generating output to keywords/6.txt\n",
      "Generating output to summaries/6.txt\n",
      "-\n",
      "Generating output to keywords/7.txt\n",
      "Generating output to summaries/7.txt\n",
      "-\n",
      "Generating output to keywords/8.txt\n",
      "Generating output to summaries/8.txt\n",
      "-\n",
      "Generating output to keywords/9.txt\n",
      "Generating output to summaries/9.txt\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "articles = os.listdir(\"articles\")\n",
    "for article in articles:\n",
    "    article_file = io.open('articles/'+article,'r',encoding=\"utf8\")\n",
    "    text = article_file.read()\n",
    "    keyphrases = extract_key_phrases(text)\n",
    "    summary = extract_sentences(text)\n",
    "    write_files(summary, keyphrases, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
